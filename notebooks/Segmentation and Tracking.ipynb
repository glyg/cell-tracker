{
 "metadata": {
  "name": "",
  "signature": "sha256:ab2fa14eaaa9c5f108448fbb11a9a7e06352aaa8b807e029073ebff6461c54f6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Initialize all we need"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "\n",
      "%matplotlib\n",
      "%run nb_init.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using matplotlib backend: Qt4Agg\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Choose the file(s) to work on"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "default_path = '/media/data/xiaobo'\n",
      "cluster = ct.ui.get_cluster(default_path, single_file=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Choosen data path: /media/data/xiaobo/Damien/Control 1.tif\n",
        "Getting metadata from TIFF file"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Set metadata correctly"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ct.ui.set_metadata(cluster.oio.metadata)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Look at the first image"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can change at other ones by changing the `stack_num` in the function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "im0 = ct.inspect_stack(cluster, stack_num=0, show=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "z stack shape: (19, 2, 644, 1018)\n",
        "maximum value: 1596\n",
        "number of unique values: 1080\n",
        "infered signal depth: 12\n",
        "Stack doesn't seem to have the correct dimensions, make sure to apply a preprocessing to correct this descrepency\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Cells nuclei detection"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Preprocessing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to pass the images to the detection process, it might be necessary to modifiy\n",
      "them. We call this _preprocessing_.\n",
      "\n",
      "You can choose a variety of preprocessing functions:\n",
      "\n",
      "* `normalize` : Usefull for stacks with very high values or strange data type (e.g. after wild-field deconvolution)\n",
      "* `highpass` : Allows to get rid of variations in the image background (for raw wild-field images for example)\n",
      "* `from16to8` : Just devides the image by 256, to pass it from 16 bits to 8. Usefull if the number of unique values\n",
      "    is much lower than the maximum value\n",
      "* `red_channel`: Returns the red channel of the stack, assuming the stack is ordered as (Z-plane, Color, X, Y)\n",
      "    similar preprocessing is available for green and blue channels.\n",
      "* `red_16to8` :  Returns the red channel devided by 256. Usefull for images that went through \n",
      "    strange conversions (from confocal    stacks for example)\n",
      "\n",
      "The cell bellow allows you to choose one of those. If it's not needed just leave `None` here.\n",
      "\n",
      "_Hint_: Try hitting the `<TAB>` key (the one with two opposite arrows) with the cursor at the dot \n",
      "after preprocess to see the possible completions\n",
      "    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "preprocess = ct.preprocess.green_channel"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Detection parameters"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ct.ui.set_parameters()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The detection itself"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Single stack detection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use this if you want to adjust the parameters."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cluster.stackio.metadata"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "{'DimensionOrder': 'TZCYX',\n",
        " 'SizeZ': 19,\n",
        " 'FileName': '/media/data/xiaobo/Damien/Control 1.tif',\n",
        " 'SizeX': 1018,\n",
        " 'SizeY': 644,\n",
        " 'Shape': [82, 19, 2, 644, 1018],\n",
        " 'SignificantBits': 12,\n",
        " 'SizeT': 82,\n",
        " 'SizeC': 2}"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stack_num = 40  #Change here to look at a different time point\n",
      "\n",
      "stack, one_stack_output = ct.detection.single_stack_detection(cluster, stack_num, preprocess)\n",
      "ct.graphics.show_one_stack_output(stack, one_stack_output)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "3D vue of the detection"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ax = ct.graphics.show_projected(cluster, stack_num,\n",
      "                                preprocess=preprocess,\n",
      "                                xy_ROI=None, ax=None,\n",
      "                                c='r', alpha=0.5, s=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Whole movie detection"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "for stk in cluster.stack_iterator():\n",
      "    print(stk.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(82, 644, 1018)\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cluster.detect_cells(preprocess=preprocess,\n",
      "                     parameters=ct.detection_parameters,\n",
      "                     parallel=False,\n",
      "                     mapper=None)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Plotting the results"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Whole trajectories"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "axes = draw.plot_stacked_coords(cluster.trajs,\n",
      "                                coords=('x', 'y'),\n",
      "                                text=False,\n",
      "                                ls='o',\n",
      "                                alpha=0.5)\n",
      "title = axes[0].set_title('After detection')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Saving"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cluster.oio['trajs'] = cluster.trajs\n",
      "cluster.oio['raw'] = cluster.trajs\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Tracking"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Some explanations on tracking parameters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* `max_speed`: This parameter controls the maximum allowed speed for a nucleus\n",
      "  between to be tracked between two frames. So if you think that, for example, a 9 \u00b5m \n",
      "  displacement in 3 minutes is much too high, put `max_speed` to 3. \n",
      "* `cut_penalty`: This is usually comprized between 1 and 2. The higher it gets, the more fragmented the\n",
      "  trajectories will be.\n",
      "\n",
      "* `max_gap`: The maximum number of frames between two trajectory segments that might be linked together\n",
      "\n",
      "* `fragment`: Between 0 and 15: the higher this gets, the more fragmented trajectories will be (less gap closing)\n",
      " "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "max_speed = 2\n",
      "cut_penalty = 1.2\n",
      "max_gap = 3\n",
      "fragment = 1.\n",
      "coords = ['x', 'y', 'z']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Trajectory reconstruction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cluster.track_cells(coords=coords,\n",
      "                    max_speed=max_speed,\n",
      "                    cut_penalty=cut_penalty,\n",
      "                    max_gap=max_gap,\n",
      "                    fragment=fragment)\n",
      "### Plot the results\n",
      "axes = draw.plot_stacked_coords(cluster.trajs,\n",
      "                                coords=('x', 'y'),\n",
      "                                text=False,\n",
      "                                alpha=0.5)\n",
      "title = axes[0].set_title('After reconstruction')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-05-25 17:26:20:INFO:sktracker.tracker.solver.gap_close_solver: Initiating gap close\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-05-25 17:26:24:INFO:sktracker.tracker.solver.gap_close_solver: 70 gap close event processed\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Choose cells to use in the analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ct.ui.pick_border_cells(cluster)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Manual correction of the trajectories"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cluster.thumbs = ct.graphics.load_thumbs(cluster, preprocess=preprocess,\n",
      "                                         reset_ROI=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mt = ct.ui.ManualTracking(cluster, t0=0, trail_length=6,\n",
      "                          preprocess=None)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Plot the results\n",
      "axes = draw.plot_3coords(cluster.trajs,\n",
      "                         text=False,\n",
      "                         alpha=0.5)\n",
      "title = axes[0].set_title('After correction')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 131
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Saving"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cluster.oio['trajs'] = cluster.trajs\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    }
   ],
   "metadata": {}
  }
 ]
}